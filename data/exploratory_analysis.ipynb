{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_json_serializable(data):\n",
    "    if isinstance(data, dict):\n",
    "        # Para diccionarios, aplica la conversión a cada valor\n",
    "        return {key: convert_to_json_serializable(value) for key, value in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        # Para listas, aplica la conversión a cada elemento\n",
    "        return [convert_to_json_serializable(element) for element in data]\n",
    "    elif isinstance(data, datetime):\n",
    "        # Convierte datetime a string usando isoformat()\n",
    "        return data.isoformat()\n",
    "    else:\n",
    "        # Para cualquier otro tipo de dato, lo devuelve sin cambios\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generado y guardado como 'dataset_riesgos_espana.parquet'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "\n",
    "faker = Faker('es_ES')\n",
    "\n",
    "# Provincias de España\n",
    "provincias = [\"Almería\", \"Cádiz\", \"Córdoba\", \"Granada\", \"Huelva\", \"Jaén\", \"Málaga\", \"Sevilla\", \"Huesca\", \"Teruel\", \"Zaragoza\", \"Asturias\", \"Islas Baleares\", \"Las Palmas\", \"Santa Cruz de Tenerife\", \"Cantabria\", \"Albacete\", \"Ciudad Real\", \"Cuenca\", \"Guadalajara\", \"Toledo\", \"Ávila\", \"Burgos\", \"León\", \"Palencia\", \"Salamanca\", \"Segovia\", \"Soria\", \"Valladolid\", \"Zamora\", \"Barcelona\", \"Girona\", \"Lleida\", \"Tarragona\", \"Badajoz\", \"Cáceres\", \"A Coruña\", \"Lugo\", \"Ourense\", \"Pontevedra\", \"Madrid\", \"Murcia\", \"Navarra\", \"Álava\", \"Bizkaia\", \"Gipuzkoa\", \"La Rioja\", \"Ceuta\", \"Melilla\", \"Alicante\", \"Castellón\", \"Valencia\"]\n",
    "\n",
    "# Riesgos, Plazos, Escenarios\n",
    "riesgos = [\"inundación\", \"riesgo sísmico\", \"ola de calor\"]\n",
    "plazos = [\"corto\", \"medio\", \"largo\"]\n",
    "escenarios = [\"optimista\", \"pesimista\"]\n",
    "\n",
    "# Generar datos\n",
    "data = []\n",
    "\n",
    "# Función para generar coordenadas de cuadrícula (usado para inundaciones)\n",
    "def generar_cuadrados(centro, num_cuadrados_por_lado, espaciado, delta):\n",
    "    puntos_cuadricula = [(i, j) for i in range(num_cuadrados_por_lado) for j in range(num_cuadrados_por_lado)]\n",
    "    cuadrados = []\n",
    "    for punto in puntos_cuadricula:\n",
    "        centro_cuadrado = centro + np.array([punto[0] * espaciado, punto[1] * espaciado]) - np.array([num_cuadrados_por_lado / 2 * espaciado, num_cuadrados_por_lado / 2 * espaciado])\n",
    "        cuadrado = [\n",
    "            [centro_cuadrado[0] + delta, centro_cuadrado[1] + delta],  # Superior derecho\n",
    "            [centro_cuadrado[0] + delta, centro_cuadrado[1] - delta],  # Superior izquierdo\n",
    "            [centro_cuadrado[0] - delta, centro_cuadrado[1] - delta],  # Inferior izquierdo\n",
    "            [centro_cuadrado[0] - delta, centro_cuadrado[1] + delta],  # Inferior derecho\n",
    "        ]\n",
    "        cuadrados.append(cuadrado)\n",
    "    return cuadrados\n",
    "\n",
    "centro = np.array([38.345996, -0.490686])  # Punto central para generación de cuadrados\n",
    "num_cuadrados_por_lado = 100  # Número de cuadrados por lado\n",
    "espaciado = 0.001  # Espaciado entre cuadrados\n",
    "delta = 0.0001  # Tamaño del desplazamiento para los vértices\n",
    "\n",
    "for provincia in provincias:\n",
    "    for riesgo in riesgos:\n",
    "        for plazo in plazos:\n",
    "            for escenario in escenarios:\n",
    "                if riesgo == \"ola de calor\":\n",
    "                    # Para olas de calor, generamos datos temporales y de temperatura\n",
    "                    fecha_hora = faker.date_time_between(start_date=\"-1y\", end_date=\"now\")\n",
    "                    temperatura = np.random.uniform(20, 45)\n",
    "                    latitud = np.random.uniform(36, 43.5)\n",
    "                    longitud = np.random.uniform(-9, 3)\n",
    "                    # Asegúrate de que los datos estén en un formato consistente, como una lista de diccionarios\n",
    "                    coordenadas = [{\"fecha_hora\": fecha_hora, \"temperatura\": temperatura, \"latitud\": latitud, \"longitud\": longitud}]\n",
    "\n",
    "                # Antes de guardar en Parquet, convertir la columna 'Coordenadas' a un string JSON si contiene listas de diccionarios\n",
    "                data.append([provincia, riesgo, plazo, escenario, coordenadas])\n",
    "\n",
    "# Crear DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Provincia\", \"Riesgo\", \"Plazo\", \"Escenario\", \"Coordenadas\"])\n",
    "df['Coordenadas'] = df['Coordenadas'].apply(lambda x: json.dumps(convert_to_json_serializable(x)))\n",
    "# Guardar como Parquet\n",
    "df.to_parquet('dataset_riesgos_espana.parquet', partition_cols=['Provincia', 'Riesgo', 'Plazo', 'Escenario'])\n",
    "\n",
    "print(\"Dataset generado y guardado como 'dataset_riesgos_espana.parquet'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowNotImplementedError",
     "evalue": "Unsupported cast from list<element: list<element: double>> to struct using function cast_struct",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowNotImplementedError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset_riesgos_espana.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpyarrow\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Master Mbit/PFM/proyecto_streamlit/venv/lib/python3.9/site-packages/pandas/io/parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Master Mbit/PFM/proyecto_streamlit/venv/lib/python3.9/site-packages/pandas/io/parquet.py:274\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[1;32m    268\u001b[0m     path,\n\u001b[1;32m    269\u001b[0m     filesystem,\n\u001b[1;32m    270\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    271\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m     result \u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mto_pandas(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_pandas_kwargs)\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/Master Mbit/PFM/proyecto_streamlit/venv/lib/python3.9/site-packages/pyarrow/parquet/core.py:1825\u001b[0m, in \u001b[0;36mread_table\u001b[0;34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[0m\n\u001b[1;32m   1813\u001b[0m     \u001b[38;5;66;03m# TODO test that source is not a directory or a list\u001b[39;00m\n\u001b[1;32m   1814\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m ParquetFile(\n\u001b[1;32m   1815\u001b[0m         source, read_dictionary\u001b[38;5;241m=\u001b[39mread_dictionary,\n\u001b[1;32m   1816\u001b[0m         memory_map\u001b[38;5;241m=\u001b[39mmemory_map, buffer_size\u001b[38;5;241m=\u001b[39mbuffer_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1822\u001b[0m         page_checksum_verification\u001b[38;5;241m=\u001b[39mpage_checksum_verification,\n\u001b[1;32m   1823\u001b[0m     )\n\u001b[0;32m-> 1825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1826\u001b[0m \u001b[43m                    \u001b[49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Master Mbit/PFM/proyecto_streamlit/venv/lib/python3.9/site-packages/pyarrow/parquet/core.py:1468\u001b[0m, in \u001b[0;36mParquetDataset.read\u001b[0;34m(self, columns, use_threads, use_pandas_metadata)\u001b[0m\n\u001b[1;32m   1460\u001b[0m         index_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1461\u001b[0m             col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m _get_pandas_index_columns(metadata)\n\u001b[1;32m   1462\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m   1463\u001b[0m         ]\n\u001b[1;32m   1464\u001b[0m         columns \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1465\u001b[0m             \u001b[38;5;28mlist\u001b[39m(columns) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(index_columns) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(columns))\n\u001b[1;32m   1466\u001b[0m         )\n\u001b[0;32m-> 1468\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filter_expression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1470\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_threads\u001b[49m\n\u001b[1;32m   1471\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1473\u001b[0m \u001b[38;5;66;03m# if use_pandas_metadata, restore the pandas metadata (which gets\u001b[39;00m\n\u001b[1;32m   1474\u001b[0m \u001b[38;5;66;03m# lost if doing a specific `columns` selection in to_table)\u001b[39;00m\n\u001b[1;32m   1475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pandas_metadata:\n",
      "File \u001b[0;32m~/Documents/Master Mbit/PFM/proyecto_streamlit/venv/lib/python3.9/site-packages/pyarrow/_dataset.pyx:562\u001b[0m, in \u001b[0;36mpyarrow._dataset.Dataset.to_table\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/Master Mbit/PFM/proyecto_streamlit/venv/lib/python3.9/site-packages/pyarrow/_dataset.pyx:3722\u001b[0m, in \u001b[0;36mpyarrow._dataset.Scanner.to_table\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/Master Mbit/PFM/proyecto_streamlit/venv/lib/python3.9/site-packages/pyarrow/error.pxi:154\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/Master Mbit/PFM/proyecto_streamlit/venv/lib/python3.9/site-packages/pyarrow/error.pxi:91\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowNotImplementedError\u001b[0m: Unsupported cast from list<element: list<element: double>> to struct using function cast_struct"
     ]
    }
   ],
   "source": [
    "data = pd.read_parquet('dataset_riesgos_espana.parquet', engine='pyarrow',\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coordenadas</th>\n",
       "      <th>provincia</th>\n",
       "      <th>riesgo</th>\n",
       "      <th>plazo</th>\n",
       "      <th>escenario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Coordenadas, provincia, riesgo, plazo, escenario]\n",
       "Index: []"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
